{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67048ef",
   "metadata": {},
   "source": [
    "Cell 1: Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7747e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3237995433.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThis notebook trains a custom YOLOv8 model to segment conjunctiva from eye images using your real dataset.\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# YOLOv8 Conjunctiva Segmentation Training\n",
    "\n",
    "This notebook trains a custom YOLOv8 model to segment conjunctiva from eye images using your real dataset.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Training images**: 100+ eye images with conjunctiva annotations\n",
    "- **Validation images**: 80+ eye images with conjunctiva annotations  \n",
    "- **Class**: Single class (conjunctiva)\n",
    "- **Format**: YOLO format with normalized coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10082ab4",
   "metadata": {},
   "source": [
    "Cell 2: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f24d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8586e7",
   "metadata": {},
   "source": [
    "Cell 3: Import YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported YOLOv8 model components\n"
     ]
    }
   ],
   "source": [
    "# Import only the model classes that exist\n",
    "from yolov8_simple import YOLOv8, YOLOConfig\n",
    "\n",
    "print(\"Successfully imported YOLOv8 model components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebcab5",
   "metadata": {},
   "source": [
    "Cell 4: Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Configuration:\n",
      "  - Training path: train/images\n",
      "  - Validation path: valid/images\n",
      "  - Test path: test/images\n",
      "  - Classes: {0: 'conjunctiva'}\n",
      "\n",
      "Dataset Statistics:\n",
      "  - Training images: 300\n",
      "  - Validation images: 100\n",
      "  - Test images: 16\n",
      "  - Total images: 416\n"
     ]
    }
   ],
   "source": [
    "# Load dataset configuration\n",
    "with open('data.yaml', 'r') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(f\"  - Training path: {dataset_config['train']}\")\n",
    "print(f\"  - Validation path: {dataset_config['val']}\")\n",
    "print(f\"  - Test path: {dataset_config['test']}\")\n",
    "print(f\"  - Classes: {dataset_config['names']}\")\n",
    "\n",
    "# Count images in each split\n",
    "train_images = len([f for f in os.listdir(dataset_config['train']) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "valid_images = len([f for f in os.listdir(dataset_config['val']) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "test_images = len([f for f in os.listdir(dataset_config['test']) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  - Training images: {train_images}\")\n",
    "print(f\"  - Validation images: {valid_images}\")\n",
    "print(f\"  - Test images: {test_images}\")\n",
    "print(f\"  - Total images: {train_images + valid_images + test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99a767",
   "metadata": {},
   "source": [
    "Cell 5: Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation configured:\n",
      "  - Training: Resize, Flip, Brightness/Contrast, Hue/Saturation, Gamma, Blur\n",
      "  - Validation: Resize only\n"
     ]
    }
   ],
   "source": [
    "def get_train_transforms(input_size=640):\n",
    "    \"\"\"Get training transforms with data augmentation\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height=input_size, width=input_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.HueSaturationValue(p=0.2),\n",
    "        A.RandomGamma(p=0.2),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "def get_val_transforms(input_size=640):\n",
    "    \"\"\"Get validation transforms (no augmentation)\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height=input_size, width=input_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "print(\"Data augmentation configured:\")\n",
    "print(\"  - Training: Resize, Flip, Brightness/Contrast, Hue/Saturation, Gamma, Blur\")\n",
    "print(\"  - Validation: Resize only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20beb1be",
   "metadata": {},
   "source": [
    "Cell 6: Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc80915",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mConjunctivaDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images_dir, labels_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m, input_size=\u001b[32m640\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mself\u001b[39m.images_dir = images_dir\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "#You might need to install this \"pip install albumentations\"\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class ConjunctivaDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None, input_size=640):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = [f for f in os.listdir(images_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images in {images_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load labels\n",
    "        label_file = img_file.rsplit('.', 1)[0] + '.txt'\n",
    "        label_path = os.path.join(self.labels_dir, label_file)\n",
    "        \n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1])\n",
    "                        y_center = float(parts[2])\n",
    "                        width = float(parts[3])\n",
    "                        height = float(parts[4])\n",
    "                        \n",
    "                        bboxes.append([x_center, y_center, width, height])\n",
    "                        class_labels.append(class_id)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=bboxes,\n",
    "                class_labels=class_labels\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            class_labels = transformed['class_labels']\n",
    "        \n",
    "        # Convert to tensor format for YOLO\n",
    "        targets = []\n",
    "        for bbox, class_id in zip(bboxes, class_labels):\n",
    "            targets.append([class_id] + bbox)\n",
    "        \n",
    "        return image, targets\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images = []\n",
    "        targets = []\n",
    "        \n",
    "        for img, target in batch:\n",
    "            images.append(img)\n",
    "            targets.extend([[len(images)-1] + t for t in target])\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "        return images, targets\n",
    "\n",
    "# Example transforms (you might have your own)\n",
    "def get_train_transforms(input_size=640):\n",
    "    return A.Compose([\n",
    "        A.Resize(input_size, input_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "def get_val_transforms(input_size=640):\n",
    "    return A.Compose([\n",
    "        A.Resize(input_size, input_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5477257",
   "metadata": {},
   "source": [
    "Cell 7: Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85cb4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m batch_size = \u001b[32m8\u001b[39m  \u001b[38;5;66;03m# Adjust based on your GPU memory\u001b[39;00m\n\u001b[32m      3\u001b[39m num_workers = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Set to 0 for Windows, increase for Linux/Mac\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m(\n\u001b[32m      6\u001b[39m     train_dataset,\n\u001b[32m      7\u001b[39m     batch_size=batch_size,\n\u001b[32m      8\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     num_workers=num_workers,\n\u001b[32m     10\u001b[39m     collate_fn=train_dataset.collate_fn,\n\u001b[32m     11\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m val_loader = DataLoader(\n\u001b[32m     15\u001b[39m     val_dataset,\n\u001b[32m     16\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData loaders created:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 8  # Adjust based on your GPU memory\n",
    "num_workers = 0  # Set to 0 for Windows, increase for Linux/Mac\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=val_dataset.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  - Training batch size: {batch_size}\")\n",
    "print(f\"  - Validation batch size: {batch_size}\")\n",
    "print(f\"  - Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches per epoch: {len(val_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nTesting data loading...\")\n",
    "for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    print(f\"  - Batch {batch_idx + 1}: images shape {images.shape}, targets shape {targets.shape}\")\n",
    "    if batch_idx >= 2:  # Test first 3 batches\n",
    "        break\n",
    "print(\"Data loading test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de555f48",
   "metadata": {},
   "source": [
    "Cell 8: Loss Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ef18f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mYOLOLoss\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m(YOLOLoss, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.config = config\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        \n",
    "        # Constants\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        obj = targets[..., 0] == 1  # in paper this is Iobj_i\n",
    "        noobj = targets[..., 0] == 0  # in paper this is Inoobj_i\n",
    "\n",
    "        # No object loss\n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (targets[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # Object loss\n",
    "        object_loss = self.bce(\n",
    "            (predictions[..., 0:1][obj]), (targets[..., 0:1][obj]),\n",
    "        )\n",
    "\n",
    "        # Box coordinate loss\n",
    "        box_loss = self.smooth_l1(\n",
    "            predictions[..., 1:5][obj], targets[..., 1:5][obj]\n",
    "        )\n",
    "\n",
    "        # Class loss\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (targets[..., 5][obj].long()),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )\n",
    "\n",
    "print(\"YOLO Loss function implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bf8a3-8e5b-4969-a990-659e5b1966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell 9: Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e41056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for conjunctiva detection\n",
    "config = YOLOConfig(\n",
    "    num_classes=1,  # Single class: conjunctiva\n",
    "    input_size=640,\n",
    "    anchors=3,\n",
    "    anchor_masks=[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  - Number of classes: {config.num_classes}\")\n",
    "print(f\"  - Input size: {config.input_size}\")\n",
    "print(f\"  - Number of anchors: {config.anchors}\")\n",
    "\n",
    "# Initialize model\n",
    "model = YOLOv8(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af722fc3",
   "metadata": {},
   "source": [
    "Cell 10: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea04d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "save_dir = 'trained_models'\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "loss_fn = YOLOLoss(config)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Learning rate: {learning_rate}\")\n",
    "print(f\"  - Number of epochs: {num_epochs}\")\n",
    "print(f\"  - Save directory: {save_dir}\")\n",
    "print(f\"  - Optimizer: Adam with weight decay\")\n",
    "print(f\"  - Loss function: YOLOLoss\")\n",
    "print(f\"  - Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa0271",
   "metadata": {},
   "source": [
    "Cell 11: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Avg Loss': f'{total_loss/num_batches:.4f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss/num_batches:.4f}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa637c",
   "metadata": {},
   "source": [
    "Cell 12: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70500e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch(model, val_loader, loss_fn, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(save_dir, 'yolov8_conjunctiva_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, best_model_path)\n",
    "        print(f\"New best model saved: {best_model_path}\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f'yolov8_conjunctiva_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(save_dir, 'yolov8_conjunctiva_final.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'input_size': config.input_size,\n",
    "    'num_classes': config.num_classes,\n",
    "    'final_train_loss': train_losses[-1],\n",
    "    'final_val_loss': val_losses[-1]\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8659099",
   "metadata": {},
   "source": [
    "Cell 13: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Val Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_losses, label='Val Loss', color='red')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4400d6",
   "metadata": {},
   "source": [
    "Cell 14: Simple Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_detect(model, image_path, config, device, conf_threshold=0.5):\n",
    "    \"\"\"Simple inference function for testing\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize\n",
    "    h, w = image.shape[:2]\n",
    "    resized = cv2.resize(image, (config.input_size, config.input_size))\n",
    "    \n",
    "    # Normalize and convert to tensor\n",
    "    resized = resized.astype(np.float32) / 255.0\n",
    "    resized = (resized - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    resized = torch.from_numpy(resized).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(resized)\n",
    "    \n",
    "    # Simple post-processing (this is a simplified version)\n",
    "    # In a real implementation, you'd need proper NMS and anchor handling\n",
    "    predictions = predictions.squeeze()\n",
    "    \n",
    "    # For now, just return the raw predictions\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "print(\"Simple inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c911131",
   "metadata": {},
   "source": [
    "Cell 15: Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b437e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for testing\n",
    "best_model_path = os.path.join(save_dir, 'yolov8_conjunctiva_best.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "\n",
    "# Test on validation images\n",
    "val_images_dir = dataset_config['val']\n",
    "val_image_files = [f for f in os.listdir(val_images_dir) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Test on first 6 validation images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, img_file in enumerate(val_image_files[:6]):\n",
    "    img_path = os.path.join(val_images_dir, img_file)\n",
    "    \n",
    "    # Load and display original image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform simple inference\n",
    "    predictions = simple_detect(model, img_path, config, device)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"{img_file}\\nPredictions shape: {predictions.shape}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efedbe5a",
   "metadata": {},
   "source": [
    "Cell 16: Save Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1398bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model information\n",
    "model_info = {\n",
    "    'model_path': final_model_path,\n",
    "    'best_model_path': best_model_path,\n",
    "    'input_size': config.input_size,\n",
    "    'num_classes': config.num_classes,\n",
    "    'device': str(device),\n",
    "    'total_params': sum(p.numel() for p in model.parameters()),\n",
    "    'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    'dataset_config': dataset_config,\n",
    "    'training_config': {\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'optimizer': 'Adam',\n",
    "        'scheduler': 'CosineAnnealingLR'\n",
    "    },\n",
    "    'final_metrics': {\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model information saved to model_info.json\")\n",
    "print(\"\\nTraining completed successfully!\")\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "print(f\"Final model saved at: {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
